{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader():\n",
    "    data = []\n",
    "    for i in range(1918):\n",
    "        X = np.load('D:\\\\实验\\\\实验数据\\\\吸收谱-cnn\\\\{}.npy'.format(i + 1))\n",
    "        data.append(X)\n",
    "    print('数据长度是 {}'.format(len(data)))\n",
    "    a = [0 for i in range(694)]  # 第一个样本的数量A\n",
    "    b = [1 for i in range(634)]  # 第二个样本的数量B\n",
    "    c = [2 for i in range(590)]  # 第三个样本的数量Normal\n",
    "    a = np.concatenate((a, b, c), axis=0)\n",
    "    labels = keras.utils.to_categorical(a)  # 生成标签\n",
    "    return data,labels,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader():\n",
    "    data = []\n",
    "    for i in range(2625):\n",
    "        X = np.load('D:\\实验\\实验数据\\高光谱-cnn\\{}.npy'.format(i + 1))\n",
    "        data.append(X)\n",
    "    print('数据长度是 {}'.format(len(data)))\n",
    "    a = [0 for i in range(895)]  # 第一个样本的数量A\n",
    "    b = [1 for i in range(819)]  # 第二个样本的数量B\n",
    "    c = [2 for i in range(911)]  # 第三个样本的数量Normal\n",
    "    a = np.concatenate((a, b, c), axis=0)\n",
    "    labels = keras.utils.to_categorical(a)  # 生成标签\n",
    "    return data,labels,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thzlabelloder():\n",
    "    a = [0 for i in range(698)]\n",
    "    b = [1 for i in range(656)]  # 第二个样本的数量B\n",
    "    c = [2 for i in range(626)]  # 第三个样本的数量Normal\n",
    "    a = np.concatenate((a, b, c), axis=0)\n",
    "    labels = keras.utils.to_categorical(a)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ggplabelloder():\n",
    "    a = [0 for i in range(896)]\n",
    "    b = [1 for i in range(855)]  # 第二个样本的数量B\n",
    "    c = [2 for i in range(913)]  # 第三个样本的数量Normal\n",
    "    a = np.concatenate((a, b, c), axis=0)\n",
    "    labels = keras.utils.to_categorical(a)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, y = dataloader() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('baseline.xlsx', header=None).values\n",
    "labels = thzlabelloder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('ggp.xlsx', header=None).values\n",
    "labels = ggplabelloder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(data, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = np.array(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x[:,:,:,17:117]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = valid_x[:,:,:,17:117]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x[:,:,:,15:215]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = valid_x[:,:,:,15:215]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_x, axis=0)\n",
    "train_std = np.std(train_x, axis=0)\n",
    "valid_mean = np.mean(valid_x, axis=0)\n",
    "valid_std = np.std(valid_x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = (train_x - train_mean) / train_std\n",
    "valid_x = (valid_x - valid_mean) / valid_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(train_x.shape[0],train_x.shape[1],1)\n",
    "valid_x = valid_x.reshape(valid_x.shape[0],valid_x.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation,Dense,Dropout,Flatten,Conv2D,MaxPooling2D,BatchNormalization,regularizers,AveragePooling2D,Conv1D\n",
    "from keras.layers import MaxPooling1D,AveragePooling1D,LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#平均高光谱处理\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 128, kernel_size = 2,input_shape = (94,1), kernel_regularizer=regularizers.l2(0.01) ,activation = 'relu'))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 2, kernel_regularizer=regularizers.l2(0.01), activation = 'relu'))\n",
    "model.add(AveragePooling1D(pool_size = 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 256))\n",
    "model.add(Dense(units = 3))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#太赫兹吸收谱平均\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 128, kernel_size = 2,input_shape = (94,1), kernel_regularizer=regularizers.l2(0.01),activation = 'relu'))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 2, kernel_regularizer=regularizers.l2(0.01), activation = 'relu'))\n",
    "model.add(AveragePooling1D(pool_size = 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 256))\n",
    "model.add(Dense(units = 3))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#高光谱和太赫兹吸收图像\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 128, kernel_size = 2, input_shape = (13,4,200), kernel_regularizer=regularizers.l2(0.05),activation = 'relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = 2, kernel_regularizer=regularizers.l2(0.05),activation = 'relu'))\n",
    "model.add(AveragePooling2D(pool_size = 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 64))\n",
    "model.add(Dense(units = 3))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False), metrics=['accuracy'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 5000, mode='auto', verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./20190731THz2D', histogram_freq=100, batch_size=128, write_graph=True, write_grads=False, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=model.fit(train_x, train_y, epochs=20,batch_size=256, validation_data=(valid_x, valid_y), callbacks=[history])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title(\"model loss and acc\")\n",
    "plt.ylabel(\"loss and acc\") \n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\",\"test\",\"train_acc\",\"test_acc\"],loc=\"upper right\")\n",
    "plt.savefig(\"1.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('20190802ggp2D.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.argmax(valid_y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "workbook=xlsxwriter.Workbook('ggp1D-output.xlsx')\n",
    "worksheet=workbook.add_worksheet()\n",
    "hang = result.shape[0]\n",
    "for col in range(int(hang)):\n",
    "        worksheet.write(col,0,result[col,])\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = valid_x\n",
    "x = np.sum(x,-1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "workbook=xlsxwriter.Workbook('thz1D-output.xlsx')\n",
    "worksheet=workbook.add_worksheet()\n",
    "hang = x.shape[0]\n",
    "lie = x.shape[1]\n",
    "for col in range(int(hang)):\n",
    "    for row in range(int(lie)):\n",
    "        worksheet.write(col,row,x[col,row])\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.argmax(output,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('20190802ggp2D.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'conv2d_1'\n",
    "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)#创建的新模型\n",
    "output = intermediate_layer_model.predict(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = a1.reshape(a1.shape[0],1,a1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.sum(a1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.sum(a1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = a1[2,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_seed_img(data):\n",
    "    for j in range(100):\n",
    "        t = data[2,:,:,j]\n",
    "        plt.imshow(t)\n",
    "        list_i = str(j)\n",
    "        if '1test1' not in os.listdir('./'):\n",
    "            os.makedirs('1test1')\n",
    "        plt.savefig('./1test1/' + '_'.join(list_i) + '.jpg',dpi = 1000)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_seed_img(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.sum(valid_x,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a1[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(aa[6,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_seed_img(data):\n",
    "    for j in range(100):\n",
    "        t = data[j,:]\n",
    "        plt.plot(t)\n",
    "        list_i = str(j)\n",
    "        if 'ggp1D-dense2' not in os.listdir('./'):\n",
    "            os.makedirs('ggp1D-dense2')\n",
    "        plt.savefig('./ggp1D-dense2/' + '_'.join(list_i) + '.jpg',dpi = 1000)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_seed_img(data):\n",
    "        plt.plot(data)\n",
    "        list_i = str(111)\n",
    "        if 'ggp1D-conv1' not in os.listdir('./'):\n",
    "            os.makedirs('ggp1D-conv1')\n",
    "        plt.savefig('./ggp1D-conv1/' + '_'.join(list_i) + '.jpg',dpi = 1000)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_seed_img(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "workbook=xlsxwriter.Workbook('ggp1D-dense2.xlsx')\n",
    "worksheet=workbook.add_worksheet()\n",
    "hang = a1.shape[0]\n",
    "lie = a1.shape[1]\n",
    "for col in range(int(hang)):\n",
    "    for row in range(int(lie)):\n",
    "        worksheet.write(col,row,a1[col,row])\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 权重提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weigth提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'dense_10'\n",
    "w1 = model.get_layer(layer_name).get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.array(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = w1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.sum(a1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.sum(a1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.sum(a1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_seed_img(data):\n",
    "        plt.plot(data)\n",
    "        list_i = str(4)\n",
    "        if 'ggp1D-dense2-weight' not in os.listdir('./'):\n",
    "            os.makedirs('ggp1D-dense2-weight')\n",
    "        plt.savefig('./ggp1D-dense2-weight/' + '_'.join(list_i) + '.jpg',dpi = 1000)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_seed_img(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "workbook=xlsxwriter.Workbook('ggp1D-dense2-weight.xlsx')\n",
    "worksheet=workbook.add_worksheet()\n",
    "hang = a1.shape[0]\n",
    "lie = a1.shape[1]\n",
    "for col in range(int(hang)):\n",
    "    for row in range(int(lie)):\n",
    "        worksheet.write(col,row,a1[col,row])\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bias提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = w1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2= w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = np.sum(a2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = np.sum(a2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "workbook=xlsxwriter.Workbook('ggp1D-dense1-bias.xlsx')\n",
    "worksheet=workbook.add_worksheet()\n",
    "hang = a2.shape[0]\n",
    "lie = a2.shape[1]\n",
    "for col in range(int(hang)):\n",
    "    for row in range(int(lie)):\n",
    "        worksheet.write(col,row,a2[col,row])\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "workbook=xlsxwriter.Workbook('ggp1D-dense2-bias.xlsx')\n",
    "worksheet=workbook.add_worksheet()\n",
    "hang = a2.shape[0]\n",
    "for col in range(int(hang)):\n",
    "        worksheet.write(col,0,a2[col,])\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_seed_img(data):\n",
    "        plt.plot(data)\n",
    "        list_i = str(1)\n",
    "        if 'ggp1D-dense2-bias' not in os.listdir('./'):\n",
    "            os.makedirs('ggp1D-dense2-bias')\n",
    "        plt.savefig('./ggp1D-dense2-bias/' + '_'.join(list_i) + '.jpg',dpi = 1000)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_seed_img(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_x1 = pd.read_excel('1.35m-平均光谱-715.xlsx', header=None).values\n",
    "train_mean1 = np.mean(train_x1, axis=0).reshape(1, -1)\n",
    "train_std1 = np.std(train_x1, axis=0).reshape(1, -1)\n",
    "train_x1 = (train_x1 - train_mean1) / train_std1\n",
    "train_x1 = train_x1.reshape(train_x1.shape[0],train_x1.shape[1],1)\n",
    "\n",
    "train_x2 = pd.read_excel('25F-平均光谱-700.xlsx', header=None).values\n",
    "train_mean2 = np.mean(train_x2, axis=0).reshape(1, -1)\n",
    "train_std2 = np.std(train_x2, axis=0).reshape(1, -1)\n",
    "train_x2 = (train_x2 - train_mean2) / train_std2\n",
    "train_x2 = train_x2.reshape(train_x2.shape[0],train_x2.shape[1],1)\n",
    "\n",
    "train_x3 = pd.read_excel('光1-平均光谱-602.xlsx', header=None).values\n",
    "train_mean3 = np.mean(train_x3, axis=0).reshape(1, -1)\n",
    "train_std3 = np.std(train_x3, axis=0).reshape(1, -1)\n",
    "train_x3 = (train_x3 - train_mean3) / train_std3\n",
    "train_x3 = train_x3.reshape(train_x3.shape[0],train_x3.shape[1],1)\n",
    "\n",
    "train_x4 = pd.read_excel('光F-平均光谱-712.xlsx', header=None).values\n",
    "train_mean4 = np.mean(train_x4, axis=0).reshape(1, -1)\n",
    "train_std4 = np.std(train_x4, axis=0).reshape(1, -1)\n",
    "train_x4 = (train_x4 - train_mean1) / train_std4\n",
    "train_x4 = train_x4.reshape(train_x4.shape[0],train_x4.shape[1],1)\n",
    "\n",
    "train_x5 = pd.read_excel('光m-平均光谱-616.xlsx', header=None).values\n",
    "train_mean5 = np.mean(train_x5, axis=0).reshape(1, -1)\n",
    "train_std5 = np.std(train_x5, axis=0).reshape(1, -1)\n",
    "train_x5 = (train_x5 - train_mean5) / train_std5\n",
    "train_x5 = train_x5.reshape(train_x5.shape[0],train_x5.shape[1],1)\n",
    "\n",
    "train_x6 = pd.read_excel('浙丝35-平均光谱-783.xlsx', header=None).values\n",
    "train_mean6 = np.mean(train_x6, axis=0).reshape(1, -1)\n",
    "train_std6 = np.std(train_x6, axis=0).reshape(1, -1)\n",
    "train_x6 = (train_x6 - train_mean6) / train_std6\n",
    "train_x6 = train_x6.reshape(train_x6.shape[0],train_x6.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pairwise_dist(x):\n",
    "    '''计算pairwise 距离, x是matrix\n",
    "    (a-b)^2 = a^w + b^2 - 2*a*b\n",
    "    '''\n",
    "    sum_x = np.sum(np.square(x), 1)\n",
    "    dist = np.add(np.add(-2 * np.dot(x, x.T), sum_x).T, sum_x)\n",
    "    return dist\n",
    "def cal_perplexity(dist, idx=0, beta=1.0):\n",
    "    '''计算perplexity, D是距离向量，\n",
    "    idx指dist中自己与自己距离的位置，beta是高斯分布参数\n",
    "    这里的perp仅计算了熵，方便计算\n",
    "    '''\n",
    "    prob = np.exp(-dist * beta)\n",
    "    # 设置自身prob为0\n",
    "    prob[idx] = 0\n",
    "    sum_prob = np.sum(prob)\n",
    "    perp = np.log(sum_prob) + beta * np.sum(dist * prob) / sum_prob\n",
    "    prob /= sum_prob\n",
    "    return perp, prob\n",
    "def seach_prob(x, tol=1e-5, perplexity=30.0):\n",
    "    '''二分搜索寻找beta,并计算pairwise的prob\n",
    "    '''\n",
    " \n",
    "    # 初始化参数\n",
    "    print(\"Computing pairwise distances...\")\n",
    "    (n, d) = x.shape\n",
    "    dist = cal_pairwise_dist(x)\n",
    "    pair_prob = np.zeros((n, n))\n",
    "    beta = np.ones((n, 1))\n",
    "    # 取log，方便后续计算\n",
    "    base_perp = np.log(perplexity)\n",
    " \n",
    "    for i in range(n):\n",
    "        if i % 500 == 0:\n",
    "            print(\"Computing pair_prob for point %s of %s ...\" %(i,n))\n",
    " \n",
    "        betamin = -np.inf\n",
    "        betamax = np.inf\n",
    "        perp, this_prob = cal_perplexity(dist[i], i, beta[i])\n",
    " \n",
    "        # 二分搜索,寻找最佳sigma下的prob\n",
    "        perp_diff = perp - base_perp\n",
    "        tries = 0\n",
    "        while np.abs(perp_diff) > tol and tries < 50:\n",
    "            if perp_diff > 0:\n",
    "                betamin = beta[i].copy()\n",
    "                if betamax == np.inf or betamax == -np.inf:\n",
    "                    beta[i] = beta[i] * 2\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamax) / 2\n",
    "            else:\n",
    "                betamax = beta[i].copy()\n",
    "                if betamin == np.inf or betamin == -np.inf:\n",
    "                    beta[i] = beta[i] / 2\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamin) / 2\n",
    " \n",
    "            # 更新perb,prob值\n",
    "            perp, this_prob = cal_perplexity(dist[i], i, beta[i])\n",
    "            perp_diff = perp - base_perp\n",
    "            tries = tries + 1\n",
    "        # 记录prob值\n",
    "        pair_prob[i,] = this_prob\n",
    "    print(\"Mean value of sigma: \", np.mean(np.sqrt(1 / beta)))\n",
    "    return pair_prob\n",
    "def pca(x, no_dims):\n",
    "    ''' PCA算法\n",
    "    使用PCA先进行预降维\n",
    "    '''\n",
    "    print(\"Preprocessing the data using PCA...\")\n",
    "    (n, d) = x.shape\n",
    "    x = x - np.tile(np.mean(x, 0), (n, 1))\n",
    "    l, M = np.linalg.eig(np.dot(x.T, x))\n",
    "    y = np.dot(x, M[:,0:no_dims])\n",
    "    return y\n",
    "def tsne(x, no_dims, initial_dims, perplexity, max_iter):\n",
    "    \"\"\"Runs t-SNE on the dataset in the NxD array x\n",
    "    to reduce its dimensionality to no_dims dimensions.\n",
    "    The syntaxis of the function is Y = tsne.tsne(x, no_dims, perplexity),\n",
    "    where x is an NxD NumPy array.\n",
    "    \"\"\"\n",
    " \n",
    "    # Check inputs\n",
    "    if isinstance(no_dims, float):\n",
    "        print(\"Error: array x should have type float.\")\n",
    "        return -1\n",
    "    if round(no_dims) != no_dims:\n",
    "        print(\"Error: number of dimensions should be an integer.\")\n",
    "        return -1\n",
    " \n",
    "    # 初始化参数和变量\n",
    "    x = pca(x, initial_dims).real\n",
    "    (n, d) = x.shape\n",
    "    initial_momentum = 0.5\n",
    "    final_momentum = 0.8\n",
    "    eta = 500\n",
    "    min_gain = 0.01\n",
    "    y = np.random.randn(n, no_dims)\n",
    "    dy = np.zeros((n, no_dims))\n",
    "    iy = np.zeros((n, no_dims))\n",
    "    gains = np.ones((n, no_dims))\n",
    " \n",
    "    # 对称化\n",
    "    P = seach_prob(x, 1e-5, perplexity)\n",
    "    P = P + np.transpose(P)\n",
    "    P = P / np.sum(P)\n",
    "    # early exaggeration\n",
    "    P = P * 4\n",
    "    P = np.maximum(P, 1e-12)\n",
    " \n",
    "    # Run iterations\n",
    "    for iter in range(max_iter):\n",
    "        # Compute pairwise affinities\n",
    "        sum_y = np.sum(np.square(y), 1)\n",
    "        num = 1 / (1 + np.add(np.add(-2 * np.dot(y, y.T), sum_y).T, sum_y))\n",
    "        num[range(n), range(n)] = 0\n",
    "        Q = num / np.sum(num)\n",
    "        Q = np.maximum(Q, 1e-12)\n",
    " \n",
    "        # Compute gradient\n",
    "        PQ = P - Q\n",
    "        for i in range(n):\n",
    "            dy[i,:] = np.sum(np.tile(PQ[:,i] * num[:,i], (no_dims, 1)).T * (y[i,:] - y), 0)\n",
    " \n",
    "        # Perform the update\n",
    "        if iter < 20:\n",
    "            momentum = initial_momentum\n",
    "        else:\n",
    "            momentum = final_momentum\n",
    "        gains = (gains + 0.2) * ((dy > 0) != (iy > 0)) + (gains * 0.8) * ((dy > 0) == (iy > 0))\n",
    "        gains[gains < min_gain] = min_gain\n",
    "        iy = momentum * iy - eta * (gains * dy)\n",
    "        y = y + iy\n",
    "        y = y - np.tile(np.mean(y, 0), (n, 1))\n",
    "        # Compute current value of cost function\n",
    "        if (iter + 1) % 100 == 0:\n",
    "            if iter > 100:\n",
    "                C = np.sum(P * np.log(P / Q))\n",
    "            else:\n",
    "                C = np.sum( P/4 * np.log( P/4 / Q))\n",
    "            print(\"Iteration \", (iter + 1), \": error is \", C)\n",
    "        # Stop lying about P-values\n",
    "        if iter == 100:\n",
    "            P = P / 4\n",
    "    print(\"finished training!\")\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\n",
    "    X1 = tt\n",
    "    X2 = intermediate_output\n",
    "    Y = tsne(X1, 2, 200, 30.0, 500)\n",
    "    Y2 = tsne(X2, 2, 6, 30.0, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    labels = pd.read_excel('label.xlsx', header=None).values\n",
    "    from matplotlib import pyplot as plt\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == 1:\n",
    "            plt.scatter(Y[i:i+1, 0], Y[i:i+1 ,1],2,color='red')\n",
    "        if labels[i] == 2:\n",
    "            plt.scatter(Y[i:i+1, 0], Y[i:i+1, 1],2,color='green')\n",
    "        if labels[i] == 3:\n",
    "            plt.scatter(Y[i:i+1, 0], Y[i:i+1 ,1],2,color='blue')\n",
    "        if labels[i] == 4:\n",
    "            plt.scatter(Y[i:i+1, 0], Y[i:i+1, 1],2,color='black')\n",
    "        if labels[i] == 5:\n",
    "            plt.scatter(Y[i:i+1, 0], Y[i:i+1, 1],2,color='yellow')\n",
    "        if labels[i] == 6:\n",
    "            plt.scatter(Y[i:i+1, 0], Y[i:i+1, 1],2,color='cyan')\n",
    "    plt.savefig(\"纹理秋葵6类之前.jpg\",dpi = 1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in range(len(labels)):\n",
    "        if labels[i] == 1:\n",
    "            plt.scatter(Y2[i:i+1, 0], Y2[i:i+1 ,1],2,color='red')\n",
    "        if labels[i] == 2:\n",
    "            plt.scatter(Y2[i:i+1, 0], Y2[i:i+1, 1],2,color='green')\n",
    "        if labels[i] == 3:\n",
    "            plt.scatter(Y2[i:i+1, 0], Y2[i:i+1 ,1],2,color='blue')\n",
    "        if labels[i] == 4:\n",
    "            plt.scatter(Y2[i:i+1, 0], Y2[i:i+1, 1],2,color='black')\n",
    "        if labels[i] == 5:\n",
    "            plt.scatter(Y2[i:i+1, 0], Y2[i:i+1, 1],2,color='yellow')\n",
    "        if labels[i] == 6:\n",
    "            plt.scatter(Y2[i:i+1, 0], Y2[i:i+1, 1],2,color='cyan')\n",
    "    plt.savefig(\"纹理秋葵6类之后.jpg\",dpi = 1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_x = pd.read_excel('预测数据.xlsx', header=None).values\n",
    "test_y = pd.read_excel('预测标签.xlsx', header=None).values\n",
    "test_x = ((test_x - train_mean) / train_std).reshape(test_x.shape[0],1005,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuchu = model.layers[19].output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuchu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict_test=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.argmax(predict_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "data_x = pd.read_excel('1227-x.xlsx', header=None).values\n",
    "data_y = pd.read_excel('1227-y.xlsx', header=None).values\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(data_x, data_y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook=xlsxwriter.Workbook('1227-5880-x.xlsx')\n",
    "worksheet=workbook.add_worksheet()\n",
    "hang = train_x.shape[0]\n",
    "lie = train_x.shape[1]\n",
    "for col in range(int(hang)):\n",
    "    for row in range(int(lie)):\n",
    "        worksheet.write(col,row,train_x[col,row])\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook=xlsxwriter.Workbook('1227-2520-x.xlsx')\n",
    "worksheet=workbook.add_worksheet()\n",
    "hang = valid_x.shape[0]\n",
    "lie = valid_x.shape[1]\n",
    "for col in range(int(hang)):\n",
    "    for row in range(int(lie)):\n",
    "        worksheet.write(col,row,valid_x[col,row])\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_x = pd.read_excel('testx.xlsx', header=None).values\n",
    "train_mean = np.mean(test1_x, axis=0).reshape(1, -1)\n",
    "train_std = np.std(test1_x, axis=0).reshape(1, -1)\n",
    "predict_test = ((test1_x - train_mean) / train_std).reshape(test1_x.shape[0],2151,1)\n",
    "predict_test=model.predict(predict_test)\n",
    "predict_test= np.argmax(predict_test,axis=1)\n",
    "workbook=xlsxwriter.Workbook('test-y.xlsx')\n",
    "worksheet=workbook.add_worksheet()\n",
    "hang = predict_test.shape[0]\n",
    "for col in range(int(hang)):\n",
    "    worksheet.write(col,0,predict_test[col])\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_x = pd.read_excel('1227-2520-x.xlsx', header=None).values\n",
    "train_mean = np.mean(test1_x, axis=0).reshape(1, -1)\n",
    "train_std = np.std(test1_x, axis=0).reshape(1, -1)\n",
    "predict_test = ((test1_x - train_mean) / train_std).reshape(test1_x.shape[0],417,1)\n",
    "predict_test=model.predict(predict_test)\n",
    "predict_test= np.argmax(predict_test,axis=1)\n",
    "workbook=xlsxwriter.Workbook('1227-2520-label.xlsx')\n",
    "worksheet=workbook.add_worksheet()\n",
    "hang = predict_test.shape[0]\n",
    "lie = predict_test.shape[1]\n",
    "for col in range(int(hang)):\n",
    "        worksheet.write(col,row,predict_test[col,row])\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
